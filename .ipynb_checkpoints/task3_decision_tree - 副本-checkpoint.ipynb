{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "\n",
    "决策树是一种树型结构的机器学习算法,它每个节点验证数据一个属性,根据该属性进行分割数据,将数据分布到不同的分支上,直到叶子节点,叶子结点上表示该样本的label. 每一条从根节点到叶子节点的路径表示分类[回归]的规则. 下面我们先来看看sklearn中决策树怎么用.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树详解\n",
    "### 信息论基础\n",
    "首先先来几个概念,我们后面介绍决策树原理的时候会提到,这里可以先扫一眼,用到的时候再回来看.\n",
    "1. 熵和信息熵\n",
    "\n",
    "熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量. 可以看出,熵表示的是体系的不确定性大小. 熵越大, 物理的不确定性越大. 1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题. 同理, 信息熵越小,数据的稳定性越好,我们更加相信此时数据得到的结论. 换言之, 我们现在目的肯定熵越小,机器学习得到的结果越准确. \n",
    "\n",
    "信息熵表示随机变量不确定性的度量,设随机标量X是一个离散随机变量，其概率分布为:\n",
    "$$P(X=x_i)=p_i, i=1,2,...,n$$\n",
    "则随机变量X的熵定义为:\n",
    "$$H(X)=-\\sum_{i=1}^{n}p_ilog{p_i}$$\n",
    "熵越大，随机变量的不确定性就越大，当$p_i=\\frac{1}{n}$时，\n",
    "随机变量的熵最大等于logn，故$0 \\leq H(P) \\leq logn$.\n",
    "\n",
    "2. 条件熵\n",
    "\n",
    "条件熵就是在给定X的条件的情况下，随机标量Y的条件，记作$H(Y|X)$，可以结合贝叶斯公式进行理解，定义如下\n",
    "$$H(Y|X)=\\sum_{i=1}^{n}p_iH(Y|X=x_i)$$\n",
    "这里$p_i=P(X=x_i),i=1,2,...,n$.\n",
    "一般在基于数据的估计中，我们使用的基于极大似然估计出来的经验熵和经验条件熵.\n",
    "\n",
    "3. 联合熵\n",
    "\n",
    "联合熵是相对两个及其以上的变量而言的, 两个变量X和Y的联合信息熵为:\n",
    "\n",
    "$$ H(X,Y)=-\\sum_x \\sum_y P(x,y)log_2[P(x,y)] $$\n",
    "\n",
    "其中: x和y是X和Y的特定值, 相应地, 是这些值一起出现的联合概率, 若为0, 则定义为0.\n",
    "\n",
    "对于两个以上的变量$X_1,...,X_n$,一般形式位:\n",
    "$$H(X_1,...,X_n)=-\\sum_{x_1}\\cdot \\cdot \\cdot\\sum_{x_n}P(x1,...,x_n)log_2[P(x_1,...,x_n)]$$\n",
    "\n",
    "性质:\n",
    "- 大于每个独立的熵\n",
    "$$H(X,Y) \\geq max(H(X),H(Y))$$\n",
    "$$H(X_1,...,X_n) \\geq max(H(X_1),...,H(X_n))$$\n",
    "- 小于独立熵的和\n",
    "$$H(X_1,...,X_n) \\leq sum(H(X_1),...,H(X_n))$$\n",
    "- 和条件熵的关系\n",
    "$$H(Y|X)=H(X,Y)-H(X)$$\n",
    "- 和互信息的关系\n",
    "$$I(Y;X)=H(X)+H(Y)-H(X,Y)=H(Y)-(H(X,Y)-H(X))$$\n",
    "\n",
    "\n",
    "4. 信息增益\n",
    "\n",
    "信息增益又叫**互信息**,它表示的是在的得知特征X的信息后,使得类Y的信息的不确定性(熵)减少的程度. \n",
    "$$g(Y,X)=H(Y)-H(Y|X)$$\n",
    "\n",
    "\n",
    "5. 基尼指数\n",
    "\n",
    "基尼指数又称基尼系数或者基尼不纯度,基尼系数是指国际上通用的、用以衡量一个国家或地区居民收入差距的常用指标. 在信息学中,例如分类问题, 假设有K个类,样本点属于第k类的概率是$p_k$,则该概率分布的基尼指数定义为:\n",
    "$$Gini(p)=\\sum_k^Kp_k(1-p_k)=1-\\sum_k^Kp_k^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树解释\n",
    "\n",
    "决策树是什么东西？就是我们平常所说的if-then条件，我们把它组合成树的结构. 决策树中有两种结点，叶子结点和非叶子结点. 其中非叶节点代表的条件，叶子结点表示的实例所属的类别. \n",
    "\n",
    "我们如何生成这个决策树呢，最主要的一点就是选择那个特征作为当前树的分割结点，这就叫做特征选择，有了特征选择就有了决策树的生成，最后我们还有进行决策树剪枝(后面会提到为什么剪枝). \n",
    "\n",
    "看个统计学习方法上的例子:\n",
    "\n",
    "现在我们有下面一张表的数据，想生成一个决策树模型，预测某个人是否符合贷款条件. \n",
    "<center><img src=\"https://i.ibb.co/xm7d5LV/Screenshot-from-2018-12-27-17-32-10.png\" border=\"0\"></center>\n",
    "\n",
    "现在假如我们通过\"某种方法\"构造了一颗下面的决策树. 从下图可以看到特征对应的是非叶子结点，如果这个被分到这个叶节点，就预测为这个叶节点的类别. 从图中我们可以知道以下两点:\n",
    "1. 每一个叶子节点都对应一条从根节点到叶节点的规则，这表示决策树是if-then规则的集合\n",
    "2. 如果一个实例到达了某个叶节点，一定表示它满足了从根节点到该叶子节点的所有条件，而后才得到类别，这不就是先满足条件再得到概率嘛，我们一般叫做条件概率分布. \n",
    "<center><img src=\"https://i.ibb.co/3TQDgwH/Screenshot-from-2018-12-27-17-47-01.png\" alt=\"Screenshot-from-2018-12-27-17-47-01\" border=\"0\"></center>\n",
    "\n",
    "> 问题来了，为什么我们要选择是否有房子作为第一个构造特征呢？我们构造学习模型，会遵守经验风险最小化或者似然函数极大规则，选择损失函数，我们如何根据风险最小化，选择特征呢？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3&C4.5\n",
    "\n",
    "### 数据\n",
    "\n",
    "给定训练数据集\n",
    "\n",
    "$$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$$\n",
    "\n",
    "其中，$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$特征向量，n是特征的个数，$y_i \\in \\{1,2,...,K\\}$表示类别. N是样本个数. 基于这个数据生成决策树模型. \n",
    "\n",
    "### 决策树\n",
    "\n",
    "常见的决策树模型有以下三种(CART决策树既可以做分类也可以做回归):\n",
    "\n",
    "```\n",
    "1. ID3: 使用信息增益准则选择特征, 相当于用极大似然法进行概率模型选择. \n",
    "2. C4.5: 和ID3算法相似, 只是用信息增益比选择特征. \n",
    "3. CART: 递归构建二叉决策树, 回归树:使用平方误差; 分类树:使用基尼指数. \n",
    "```\n",
    "\n",
    "| model | feature select | 树的类型 |计算公式|\n",
    "|:-----:|:--------------:|:------:|:------:|\n",
    "|ID3    |{分类:信息增益}|多叉树|$g(D,A)=H(D)-H(D\\|A)$|\n",
    "|C4.5   |{分类:信息增益比}|多叉树|$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$|\n",
    "|CART   |{回归:平方误差;分类:基尼指数}|二叉树|$Gini(p)=\\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_k^2$|\n",
    "\n",
    "其中, $H_A(D)=H(D|A)$.\n",
    "\n",
    "从表格中，我们总结(ID3,C4.5)决策树算法伪代码:\n",
    "1. 输入:数据集D，特征集合A，阈值e\n",
    "2. 输出:决策树T\n",
    "3. 如果D中所有实例输出同一类$C_k$, 则T作为单节点树，并将类$C_k$作为该节点的类标记，返回T;\n",
    "4. 若$A=\\varnothing$,则T为单节点树，将D中实例数最多的类$C_k$作为该节点的类标记，返回T；\n",
    "5. 否则，根据**信息增益**(ID3)或者**信息增益比**(C4.5)计算特征A对D的值，选择当前最优的特征$A_g$;\n",
    "6. 如果$A_g$的信息增益小于阈值e，则置T为单节点数，并将D中实例最多的类$C_k$作为当前的类标记，返回T；\n",
    "7. 否则，根据$A_g$中的每一个不同的$a_i$,根据$A_g=a_i$将D分成若干个非空子集，对于第i个子节点，以$D_i$为数据集，以$A-{A_g}$为特征集，递归(重复3-6)构造决策树$T_i$,返回$T_i$.\n",
    "8. 对决策树模型T进行剪枝.\n",
    "\n",
    "### 过拟合和剪枝\n",
    "\n",
    "决策树建立的过程中,只考虑经验损失最小化,没有考虑结构损失. 因此可能在训练集上表现良好,但是会出现过拟合问题.(我们构造决策树的时候，是完全的在训练数据上得到最优的模型. 这就是过拟合问题，训练误差很小，但是验证集上就不怎么好用.) 为了解决过拟合,我们从模型损失进行考虑:\n",
    "\n",
    "$$模型损失=经验风险最小化+正则项=结构风险最小化$$\n",
    "\n",
    "思路很简单,给损失函数加上正则项再进行优化. 正则项表示树节点的个数,因此有如下公式:\n",
    "\n",
    "$$C_{\\alpha}(T)=C(T)+\\alpha|T|$$\n",
    "\n",
    "进一步详细定义,解决问题:\n",
    "\n",
    "\n",
    "重新定义损失函数，树的叶子节点个数|T|,t是树T的叶节点，该叶节点有$N_t$个样本，其中k类的样本点有$N_{tk}$个，k=1,2，...,K, $H_t(T)$是叶子节点t经验熵，$\\alpha \\leq 0$是参数,平衡经验损失和正则项，得到计算公式如下:\n",
    "$$C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha|T|$$\n",
    "其中，经验熵为:\n",
    "$$H_t(T)=-\\sum_{k}\\frac{N_{tk}}{H_t}log\\frac{N_{tk}}{H_t}$$\n",
    "这是有:\n",
    "$$C_{\\alpha}=C(T)+\\alpha|T|$$\n",
    "决策树剪枝优化过程考虑了在训练数据上的经验风险最小化和减小模型复杂度两个方向. 因为加了正则项，所有我们基于贪心的思想进行剪枝，因为当剪掉一个树节点，虽然经验风险增加了，但是模型复杂度降低了，我们基于两个方面的平衡进行剪枝，如果剪掉之后，总的风险变小，就进行剪枝.       \n",
    "算法:       \n",
    "输入: 算法产生的整个决策树，参数$\\alpha$        \n",
    "修剪之后的树$T_{\\alpha}$        \n",
    "1. 计算每个节点的经验熵\n",
    "2. 递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\\alpha}(T_B)$和$C_{\\alpha}(T_A)$，如果:\n",
    "   $$C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B)$$\n",
    "    表示，剪掉之后，损失减小，就进行剪枝.\n",
    "3. 重复2，直到不能继续为止，得到损失函数最小的子树$T_{\\alpha}$. \n",
    "\n",
    "\n",
    "\n",
    "**4. 动态规划剪枝**.\n",
    "\n",
    "可以看出来上述算法是一个递归问题,存在很多重复项计算,这里我们使用dfs+备忘录进行加速计算,这种方法和动态规划类似.\n",
    "\n",
    "算法:       \n",
    "输入: 算法产生的整个决策树，参数$\\alpha$        \n",
    "修剪之后的树$T_{\\alpha}$ \n",
    "1. dp[所有树的节点] = {0}; 保留所有几点的信息熵\n",
    "2. 计算每个cur_node节点的经验熵, {if dp[cur_node] 直接返回, 否则, 执行2}\n",
    "3. 递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\\alpha}(T_B)$和$C_{\\alpha}(T_A)$，如果:\n",
    "   $$C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B)$$\n",
    "    表示，剪掉之后，损失减小，就进行剪枝.\n",
    "    \n",
    "$$dp[cur_node] = C_{\\alpha}(T_A)$$\n",
    "4. 重复2，直到不能继续为止，得到损失函数最小的子树$T_{\\alpha}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "分类与回归树(classification and regression tree, CART)与上述决策树的不同，\n",
    "1. 既可以做分类又可以做回归. \n",
    "2. 是二叉树，内部节点特征取值，只有yes和no两个选项      \n",
    "同样地，先进行决策树构造，在基于验证集上进行CART决策树的剪枝，既然是回归和分类树，我们就分别介绍回归和分类两种情况.\n",
    "+ 分类: gini指数\n",
    "+ 回归: 平方误差\n",
    "定义数据格式:\n",
    "$$D={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$\n",
    "其中，$x_i$是向量，当回归问题时，$y_i$是连续变量; 分类问题时，$y_i$是离散变量. \n",
    "\n",
    "### 回归树(Regerssion Tree)\n",
    "算法:       \n",
    "在训练数据上，根据某一个特征将每个区域划分为两个子区域并决定每个子区域的输出值，递归构建二叉树.\n",
    "1. 选择最优切分变量j和切分点s，求解\n",
    "$$min_{j,s}[min_{c_1}\\sum_{x_i \\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "遍历变量j，对固定的切分变量j扫描所有的s，找到使得上式最小的对(j,s).\n",
    "2. 使用选定的(j,s)划分区域并决定相应的输出值:\n",
    "   $$R_1(j,s)=\\{x|x^{(j)} \\leq s \\}, R_2(j,s)=\\{x|x^{(j)} > s \\},$$\n",
    "   $$c_m=\\frac{1}{N_m}\\sum_{x_i \\in R_m(j,s)}y_i, x \\in R_m, m=1,2$$\n",
    "3. 继续对两个子区域调用1和2，知道满足条件\n",
    "4. 将输入空间划分为M个区域$R_1,R_2,...,R_m$,生成决策树:\n",
    "   $$f(x)=\\sum_{m=1}^{M}c_mI(x \\in R_m)$$\n",
    "\n",
    "### 分类树(classification tree)\n",
    "基尼指数计算公式如下:\n",
    "$$Gini(p)=\\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_k^2$$\n",
    "基于数据D，得到:\n",
    "$$Gini(D)=1-\\sum_{k=1}^{K}p_k^2$$\n",
    "其中,$C_k$是D中所属第k类的样本子集，K是类的个数.        \n",
    "如果样本集合D根据特征A是否取某一可能取值a被被划分成$D_1$和$D_2$两个部分.\n",
    "$$D_1=\\{(x,y) \\in D | A(x)=a \\}, D_2= D-D_1$$\n",
    "在特征A的条件下，集合D的基尼指数定义为:\n",
    "$$Gini(D,A)=\\frac{|D_1|}{D}Gini(D_1)+\\frac{|D_2|}{D}Gini(D_2)$$\n",
    "基尼指数和熵一样，同样表示集合D的不确定性，基尼指数(Gini(D,A))表示根据调教A=a后的集合D的不确定性，基尼指数越大，表示数据D的不确定性越大.\n",
    "\n",
    "算法:       \n",
    "输入:训练数据D，停止计算的条件      \n",
    "输出:CART决策树     \n",
    "1. 计算所有特征A的每一个值a对应的条件基尼指数的值，选择最优的划分得到$D_1$和$D_2$.\n",
    "2. 递归对两个数据集$D_1$和$D_2$继续调用1，知道满足条件.\n",
    "3. 生成CART树的分类树. \n",
    "4. 预测的时候，根据决策树，x落到的叶子节点对应的类别表示这个预测x的类别.\n",
    "\n",
    "### CART剪枝\n",
    "从上面两个算法的不同可以看出，只是在label的设置和决策节点选择的方式不同，整个架构依然是决策树的常用的架构. 而且上面的树的构建过程，都是基于训练数据的经验风险最小化，没有使用带有正则项的结构风险最小化，这样的模型容易发生过拟合，为了不让模型过拟合，我们需要进行模型的剪枝.\n",
    "\n",
    "**CART树的剪枝有很多难点和有意思的地方让我们开慢慢解开这层面纱**\n",
    "CART剪枝算法有两步组成:\n",
    "1. 从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\\{T_0,T_1,...,T_n\\}$.\n",
    "2. 通过交叉验证的方法在独立的验证数据集上堆子序列进行测试，得到最优子树\n",
    "\n",
    "损失函数:\n",
    "$$C_{\\alpha}(T)=C(T)+\\alpha|T|$$\n",
    "其中，T为任意子树，$C(T)$是在训练数据上的预测误差，|T|是树的叶子节点的个数,$\\alpha \\geq 0$是参数，$C_{\\alpha}(T)$是参数$\\alpha$是的子树T的整体的损失，参数$\\alpha$是平衡训练数据拟合程度和模型复杂度的权重.             \n",
    "对于固定的$\\alpha$,一定存在使损失函数$C_{\\alpha}(T)$最小的子树，将其记作$T_{\\alpha}$. 我们可以理解为每一个$\\alpha$都对应一个最有子树和最小损失. \n",
    "\n",
    "**而且**已经得到证明，可以用递归的方法对树进行剪枝. 将$\\alpha$从小增大，$0=\\alpha_0<\\alpha_1<...<\\alpha_n<+\\infty$,产生一系列的区间$[\\alpha_i,\\alpha_{i+1}),i=0,1,...,n$；剪枝得到的子树序列对应着区间$\\alpha \\in [\\alpha_i,\\alpha_{i+1}),i=0,1,...,n$的最有子树序列$\\{T_0,T_1,...,T_n\\}$. \n",
    "\n",
    "我们给出算法:       \n",
    "输入: CART算法生成的决策树T_0.      \n",
    "输出: 最优的决策树T_{\\alpha}        \n",
    "1. 设k=0, T=$T_0$.\n",
    "2. 设 $\\alpha=+\\infty$.\n",
    "3. 自下而上地对各个**内部节点t**计算$C(T_t),|T_t|$以及\n",
    "   $$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$$\n",
    "   $$\\alpha=min(\\alpha,g(t))$$\n",
    "    这里，$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶子节点个数.\n",
    "4. 自上而下地访问内部节点t，如果有$g(t)=\\alpha$,进行剪枝，并堆叶节点t以多数表决方法决定其类(分类，回归使用平均值)，得到树T.\n",
    "5. 设$k=k+1,\\alpha=\\alpha,T_k=T$.\n",
    "6. 如果T不是由根节点单独构成的树，则回到步骤4. \n",
    "7. 采用交叉验证法在子树序列$\\{T_0,T_1,...,T_n\\}$中选取最优子树$T_{\\alpha}$.\n",
    "\n",
    "接下面，我们不去看算法，来看书中给的一段文字的截图，这里截图是因为我要画图，进行比较解释，图中自由理论(哈哈):\n",
    "<center><img src=\"https://i.ibb.co/RjszN94/cart.png\" alt=\"cart\" border=\"0\"></center>\n",
    "\n",
    "看懂这个图之后，再看算法一气呵成，因为我们假设每一次得到的树都有可能是最优的，所以不能直接去最后一个树，要使用交叉验证选择最有的决策树结构. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题精选\n",
    "1. 决策树和条件概率分布的关系？\n",
    "> 决策树可以表示成给定条件下类的条件概率分布. 决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大.\n",
    "2. 为什么使用信息增益，越大越能得到好的模型？\n",
    "> 上面提到过，信息熵表示数据的混乱的程度，信息增益是信息熵和条件信息熵的差值，表示的熵减少的程度，信息增益越大，代表根据我们的决策树规则得到的数据越趋向于规整，这就是我们划分类别的目的. \n",
    "3. 为什么从信息增益变到信息增益比，目的何在？\n",
    "> 信息增益根据特征之后的条件信息熵，这样的话偏向于特征取值较多的特征的问题，因此使用信息增益比对这个问题进行校正. \n",
    "4. 为什么要进行剪枝？\n",
    "> 在构造决策树的过程中，我们的两个停止条件是，子集只有一个类别和没有可以选择的特征，这是我们全部利用了数据中的所有可以使用的信息，但是我们知道数据是可能有误差了，而且数据不足，我们需要得到更鲁棒的模型，剪枝的意义就是是的深度减小，这样的就相当于减少规则的情况下，决策树的性能反而不差，使其更加鲁棒.\n",
    "5. ID3和C4.5算法可以处理实数特征吗？如果可以应该怎么处理？如果不可以请给出理由？\n",
    "> ID3和C4.5使用划分节点的方法分别是信息增益和信息增益比，从这个公式中我们可以看到 这是处理类别特征的方法，实数特征能够计算信息增益吗？我们可以定义X是实数特征的信息增益是，$$G(D|X:t)=H(D)-H(D|X:t)$$\n",
    "其中\n",
    "$$H(D|X:t)=H(D|x \\leq t)p(x \\leq t)+H(D|x>t)p(x>t)$$ $$G(D|X)=max_t=G(D|X:t)$$\n",
    "对于每一个实数可以使用这种方式进行分割. 除此之外,我们还可以使用特征的分桶，将实数特征映射到有限个桶中，可以直接使用ID3和C4.5算法.\n",
    "6. 基尼系数存在的问题? \n",
    "> 基尼指数偏向于多值属性;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 决策树参数\n",
    "\n",
    "我们掌握理论之后,就去看看sklearn中决策树的实现.\n",
    "\n",
    "DecisionTreeClassifier: sklearn中多分类决策树的接口.\n",
    "\n",
    "Paramters: \n",
    "```\n",
    "criterion : str, 可选参数(default=\"gini\")\n",
    "    这个参数表示使用什么度量划分的质量. gini: 表示使用基尼指数.\n",
    "    entropy: 表示使用的是信息增益.\n",
    "splitter : str, optional(default=\"best\")\n",
    "    选择分割节点的策略. 支持最优(best)和随机(random)两种方式.\n",
    "\n",
    "max_depth : int or None, optional(dafault=None)\n",
    "    表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,\n",
    "    直到叶子节点的个数小于min_samples_split个数.\n",
    "min_samples_split : int, float, optional(default=2)\n",
    "    表示最小分割样例数.\n",
    "    if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.\n",
    "    if float, 表示的比例[0,1],最小的分割数字=ceil(min_samples_split * n_samples)\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "    表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.\n",
    "    if int, min_samples_leaf就是最小样例数.\n",
    "    if float, 表示的比例[0,1],最小的样例数=ceil(min_samples_leaf * n_samples)\n",
    "    \n",
    "min_weight_fraction_leaf : float, optional (default=0.) \n",
    "    \n",
    "max_features : int, float, str or None, optional(default=None)\n",
    "    进行最优分割时,特征的最大个数.\n",
    "    if int, max_features是每次分割的最大特征数\n",
    "    if float, int(max_features * n_features)作为最大特征数\n",
    "    if \"auto\", 则max_features=sqrt(n_features)\n",
    "    if \"sqrt\", 则max_features=sqrt(n_features)\n",
    "    if \"log2\", 则max_features=log2(n_features)\n",
    "    if None, 则max_features=n_features\n",
    "    \n",
    "random_state : int, RandomState instance or None, optional (default=None)\n",
    "    随机化种子, if None,使用np.random随机产生\n",
    "    \n",
    "max_leaf_nodes : int or None, optional (default=None)\n",
    "    最大的叶子节点个数,如果大于这个值,需要进行继续划分. None则表示没有限制.\n",
    "\n",
    "min_impurity_decrease : float, optional (default=0.)\n",
    "    分割之后基尼指数大于这个数,则进行分割.\n",
    "    N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                    - N_t_L / N_t * left_impurity)\n",
    "                    \n",
    "min_impurity_split : float, default=1e-7\n",
    "    停止增长的阈值,小于这个值直接返回.\n",
    "```\n",
    "\n",
    "DecisionTreeRegressor: sklearn中回归树的接口.\n",
    "```\n",
    "criterion : str, optional (default=”mse”)\n",
    "    其他参数和分类树类似.\n",
    "    mse: mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node,\n",
    "    mae: mean absolute error, which minimizes the L1 loss using the median of each terminal node.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现\n",
    "\n",
    "使用cart树的分类和回归两个接口,接口参考sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numbers\n",
    "import warnings\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"自定的树结构,用来保存决策树.\n",
    "    \n",
    "    Paramters:\n",
    "    ----------\n",
    "    col: int, default(-1)\n",
    "        当前使用的第几列数据\n",
    "    \n",
    "    val: int or float or str, 分割节点\n",
    "        分割节点的值, \n",
    "        int or float : 使用大于进行比较 \n",
    "        str : 使用等于模式\n",
    "    \n",
    "    LeftChild: DecisionTree\n",
    "        左子树, <= val\n",
    "    \n",
    "    RightChild: DecisionTree\n",
    "        右子树, > val\n",
    "    \n",
    "    results: \n",
    "    \"\"\"\n",
    "    def __init__(self, col=-1, val=None, LeftChild=None, RightChild=None, result=None):\n",
    "        self.col = col\n",
    "        self.val = val\n",
    "        self.LeftChild = LeftChild\n",
    "        self.RightChild = RightChild\n",
    "        self.result = result\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(object):\n",
    "    \"\"\"使用基尼指数的分类决策树接口.\n",
    "    \n",
    "    Paramters:\n",
    "    ---------\n",
    "    max_depth : int or None, optional(dafault=None)\n",
    "        表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,\n",
    "        直到叶子节点的个数小于min_samples_split个数.\n",
    "\n",
    "    min_samples_split : int, optional(default=2)\n",
    "        表示最小分割样例数.\n",
    "        if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.\n",
    "\n",
    "    min_samples_leaf : int, optional (default=1)\n",
    "        表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.\n",
    "        if int, min_samples_leaf就是最小样例数.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        分割之后基尼指数大于这个数,则进行分割.\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                        - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    min_impurity_split : float, default=1e-7\n",
    "        停止增长的阈值,小于这个值直接返回.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape (n_classes,) or a list of such arrays\n",
    "        表示所有的类\n",
    "    \n",
    "    feature_importances_ : ndarray of shape (n_features,)\n",
    "        特征重要性, 被选择最优特征的次数,进行降序.\n",
    "    \n",
    "    tree_ : Tree object\n",
    "        The underlying Tree object.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=1e-7):        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.classes_ = None\n",
    "        self.max_features_ = None\n",
    "        self.decision_tree = None\n",
    "        self.all_feats = None\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, check_input=True):\n",
    "        \"\"\"使用X和y训练决策树的分类模型.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "            \n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        check_input : bool, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        if isinstance(X, list):\n",
    "            X = self.__check_array(X)\n",
    "        if isinstance(y, list):\n",
    "            y = self.__check_array(y)\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"输入的数据X和y长度不匹配\")\n",
    "        \n",
    "        self.classes_ = list(set(y))\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        \n",
    "        data_origin = np.c_[X, y]\n",
    "#         print (data_origin)\n",
    "        self.all_feats = [i for i in range(X.shape[1])]\n",
    "        self.max_features_ = X.shape[0]\n",
    "        \n",
    "        data = copy.deepcopy(data_origin)\n",
    "        self.decision_tree = self.__build_tree(data, 0)\n",
    "\n",
    "\n",
    "    def __predict_one(self, input_x):\n",
    "        \"\"\"预测一个样例的返回结果.\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        input_x : list or np.ndarray\n",
    "            需要预测输入数据\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        class : 对应的类\n",
    "        \"\"\"\n",
    "        \n",
    "        tree = self.decision_tree\n",
    "        #============================= show me your code =======================\n",
    "        # here\n",
    "        pre_y = \n",
    "        #============================= show me your code =======================\n",
    "        return pre_y\n",
    "    \n",
    "    \n",
    "    def predict(self, test):\n",
    "        \"\"\"预测函数,\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        test: {array-like} of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        result : np.array(list) \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(len(test)):\n",
    "            result.append(self.__predict_one(test[i]))\n",
    "        return np.array(result)\n",
    "    \n",
    "    \n",
    "    def score(self, vali_X, vali_y):\n",
    "        \"\"\"验证模型的特征,这里使用准确率.\n",
    "        Parameters\n",
    "        ----------\n",
    "        vali_X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "\n",
    "        vali_y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        score : float, 预测的准确率\n",
    "        \"\"\"\n",
    "        vali_y = np.array(vali_y)\n",
    "        pre_y = self.predict(vali_X)\n",
    "        pre_score = 1.0 * sum(vali_y == pre_y) / len(vali_y)\n",
    "        return pre_score\n",
    "    \n",
    "    \n",
    "    def __build_tree(self, data, depth):\n",
    "        \"\"\"创建决策树的主要代码\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        data : {array-like} of shape (n_samples, n_features) + {label}\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "        \n",
    "        depth: int, 树的深度\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        DecisionTree\n",
    "            \n",
    "        \"\"\"        \n",
    "        labels = np.unique(data[:,-1])\n",
    "        # 只剩下唯一的类别时,停止,返回对应类别\n",
    "        if len(labels) == 1:\n",
    "            return DecisionTree(result=list(labels)[0])\n",
    "        \n",
    "        # 遍历完所有特征时,只剩下label标签,就返回出现字数最多的类标签\n",
    "        if not self.all_feats:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 超过最大深度,则停止,使用出现最多的参数作为该叶子节点的类\n",
    "        if self.max_depth and depth > self.max_depth:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 如果剩余的样本数大于等于给定的参数 min_samples_split,\n",
    "        # 则不在进行分割, 直接返回类别中最多的类,该节点作为叶子节点\n",
    "        if self.min_samples_split >= data.shape[0]:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 叶子节点个数小于指定参数就进行返回,叶子节点中的出现最多的类\n",
    "        if self.min_samples_leaf >= data.shape[0]:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "        \n",
    "        # 根据基尼指数选择每个分割的最优特征\n",
    "        best_idx, best_val, min_gini = self.__getBestFeature(data)\n",
    "#         print (\"Current best Feature:\", best_idx, best_val, min_gini)\n",
    "        # 如果当前的gini指数小于指定阈值,直接返回\n",
    "        if min_gini < self.min_impurity_split:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "        \n",
    "        leftData, rightData = self.__splitData(data, best_idx, best_val)\n",
    "        \n",
    "        #============================= show me your code =======================\n",
    "        # here\n",
    "        \n",
    "        #============================= show me your code =======================\n",
    "        \n",
    "        return DecisionTree(col=best_idx, val=best_val, LeftChild=leftDecisionTree, RightChild=rightDecisionTree)\n",
    "\n",
    "    \n",
    "    def __getBestFeature(self, data):\n",
    "        \"\"\"得到最优特征对应的列\n",
    "        Paramters:\n",
    "        ---------\n",
    "        data: np.ndarray\n",
    "            从data中选择最优特征\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        bestInx, val, 最优特征的列的索引和使用的值.\n",
    "        \"\"\"\n",
    "        best_idx = -1\n",
    "        best_val = None\n",
    "        min_gini = 1.0                \n",
    "        # 遍历现在可以使用的特征列\n",
    "        #============================= show me your code =======================\n",
    "\n",
    "        # here\n",
    "        \n",
    "        #============================= show me your code =======================\n",
    "        # 删除使用过的特征\n",
    "        self.all_feats.remove(best_idx)\n",
    "        \n",
    "        return best_idx, best_val, min_gini\n",
    "        \n",
    "    \n",
    "    def gini(self, labels):\n",
    "        \"\"\"计算基尼指数.\n",
    "        \n",
    "        Paramters:\n",
    "        ----------\n",
    "        labels: list or np.ndarray, 数据对应的类目集合.\n",
    "        \n",
    "        Returns: \n",
    "        -------\n",
    "        gini : float ``` Gini(p) = \\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_k^2 ```\n",
    "        \n",
    "        \"\"\"\n",
    "        #============================= show me your code =======================\n",
    "\n",
    "        # here\n",
    "        \n",
    "        #============================= show me your code =======================\n",
    "        return gini\n",
    "    \n",
    "    \n",
    "    def __splitData(self, data, featColumn, val):\n",
    "        '''根据特征划分数据集分成左右两部分.\n",
    "        Paramters:\n",
    "        ---------\n",
    "        data: np.ndarray, 分割的数据\n",
    "        \n",
    "        featColumn : int, 使用第几列的数据进行分割\n",
    "        \n",
    "        val : int or float or str, 分割的值\n",
    "            int or float : 使用比较方式\n",
    "            str : 使用相等方式\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        leftData, RightData\n",
    "            int or left: leftData <= val < rightData\n",
    "            str : leftData = val and rightData != val\n",
    "        '''\n",
    "        if isinstance(val, str):\n",
    "            leftData = data[data[:, featColumn] == val]\n",
    "            rightData = data[data[:, featColumn] != val]\n",
    "        elif isinstance(val, int) or isinstance(val, float):\n",
    "            leftData = data[data[:, featColumn] <= val]\n",
    "            rightData = data[data[:, featColumn] > val]\n",
    "        return leftData, rightData\n",
    "    \n",
    "    \n",
    "    def __check_array(self, X):\n",
    "        \"\"\"检查数据类型\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        \n",
    "        Retures\n",
    "        -------\n",
    "        X: {array-like} of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        if isinstance(X, list):\n",
    "            X = np.array(X)\n",
    "        if not isinstance(X, np.ndarray) and not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"输出数据不合法,目前只支持np.ndarray or pd.DataFrame\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DecisionTree object at 0x7f6b1339cd68>\n",
      "Classifier Score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 分类树\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print (\"Classifier Score:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. 统计学习方法-决策树\n",
    "2. https://blog.csdn.net/fool_ran/article/details/86257713\n",
    "3. https://baike.baidu.com/item/%E7%86%B5/101181?fr=aladdin\n",
    "4. https://github.com/datawhalechina/Datawhale_Learning/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1/Task3.md\n",
    "5. https://baike.baidu.com/item/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0?fromtitle=%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0&fromid=360504\n",
    "6. https://baike.baidu.com/item/%E8%81%94%E5%90%88%E7%86%B5/22709235?fr=aladdin\n",
    "7. https://github.com/datawhalechina/Daily-interview/blob/master/machine-learning/DecisionTree.md\n",
    "8. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "9. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "10. https://github.com/michaeldorner/DecisionTrees/blob/master/03_Python%20Code/implementation.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
