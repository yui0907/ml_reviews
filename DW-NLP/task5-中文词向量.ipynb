{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以头条新闻的分类数据集为语料来实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "#### 读取数据与分词\n",
    "\n",
    "- 用两种工具处理：jieba和thulac,进行分词\n",
    "- 然后去除停用词，用的百度停用词数据集\n",
    "\n",
    "**Todo**：\n",
    "\n",
    "    1.分词的效果不咋样啊，很多专有名词识别不出来\n",
    "    2.类别不均衡，需要增强\n",
    "\n",
    "本次只比较不同词向量的效果，暂时不做这些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:47:32.682928Z",
     "start_time": "2020-07-06T15:47:31.126063Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "import thulac\n",
    "import re \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:47:35.326254Z",
     "start_time": "2020-07-06T15:47:34.586868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "filename = 'toutiao_cat_data.txt'\n",
    "thu1 = thulac.thulac(seg_only=True) \n",
    "stopwords =[ line.rstrip() for line in open('stopwords-master/baidu_stopwords.txt',encoding='utf-8') ]\n",
    "\n",
    "def jieba_pre(text):\n",
    "    seg_list = jieba.cut(text, cut_all=False,HMM=True)\n",
    "    return [seg for seg in seg_list if seg not in stopwords]\n",
    "\n",
    "def thu_pre(text):\n",
    "     #默认模式是分词+词性标注，这里只分词\n",
    "    seg_list = thu1.cut(text, text=True)  #进行一句话分词\n",
    "    return [seg for seg in seg_list.split(' ')  if seg not in stopwords]\n",
    "\n",
    "def preprocess(pre_model,filename):\n",
    "    texts,labels =[],[]\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f.readlines():\n",
    "            s = l.strip().split('_!_')\n",
    "            if len(s) >=4 :\n",
    "                ss = re.sub('[0-9]+',' ',s[3])\n",
    "                texts.append(pre_model(ss)+s[4].split(','))\n",
    "                labels.append(s[1])\n",
    "    return texts,labels \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:53.663610Z",
     "start_time": "2020-07-06T15:47:44.898601Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yui\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.704 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba_texts,jieba_labels = preprocess(jieba_pre,filename)\n",
    "thu_texts,thu_labels = preprocess(thu_pre,filename)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:26:57.677158Z",
     "start_time": "2020-07-06T12:23:46.631251Z"
    }
   },
   "outputs": [],
   "source": [
    "#jieba_texts,jieba_labels = preprocess(jieba_pre,filename)\n",
    "#thu_texts,thu_labels = preprocess(thu_pre,filename)\n",
    "'''\n",
    "好慢！以防万一导出到本地\n",
    "\n",
    "jieba_seq=[]\n",
    "for i in range(len(jieba_texts)):\n",
    "    jieba_seq.append(','.join(jieba_texts[i])+','+jieba_labels[i])\n",
    "\n",
    "thu_seq=[]\n",
    "for i in range(len(jieba_texts)):\n",
    "    thu_seq.append(','.join(thu_texts[i])+','+thu_labels[i])\n",
    "\n",
    "    \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.DataFrame(data=np.array([jieba_seq,thu_seq]).transpose(),columns=['jieba_seq','thu_seq'])\n",
    "df.to_csv('fenci_pre.csv',encoding='utf-8')\n",
    "\n",
    "\n",
    "df = pd.read_csv('fenci_pre.csv',encoding='utf-8')\n",
    "jieba_texts,jieba_labels =[],[]\n",
    "thu_texts,thu_labels=[],[]\n",
    "for i in range(df.shape[0]):\n",
    "    jieba_texts.append(df.iloc[i][1].split(',')[:-1])\n",
    "    thu_texts.append(df.iloc[i][2].split(',')[:-1])\n",
    "    jieba_labels.append(df.iloc[i][1].split(',')[-1])\n",
    "    thu_labels.append(df.iloc[i][2].split(',')[-1])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:43:08.244665Z",
     "start_time": "2020-07-06T06:43:08.230607Z"
    }
   },
   "source": [
    "#### 词表字典\n",
    "\n",
    "这一步得到三个产物：\n",
    "\n",
    "    单词计数\n",
    "    单词到索引\n",
    "    索引到单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:53.670732Z",
     "start_time": "2020-07-06T15:57:53.664577Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    word_counts = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "\n",
    "\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            if word in word_counts and len(word)>1  :\n",
    "                word_counts[word]+=1\n",
    "            elif len(word)>1 :\n",
    "                word_counts[word] = 1 \n",
    "    word_counts=dict(sorted(word_counts.items(),key=lambda x:x[1],reverse=True)[:2000])\n",
    "    for  idx,item in enumerate(word_counts.items()):\n",
    "        word_to_index[item[0]] = idx \n",
    "    \n",
    "    index_to_word = dict([ (v,k) for k,v in word_to_index.items() ])\n",
    "\n",
    "    return word_counts,word_to_index,index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.436945Z",
     "start_time": "2020-07-06T15:57:53.672554Z"
    }
   },
   "outputs": [],
   "source": [
    "jb_word_counts,jb_word_to_index,jb_index_to_word = get_vocab(jieba_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.441644Z",
     "start_time": "2020-07-06T15:57:55.436945Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#thu_word_counts,thu_word_to_index,thu_index_to_word = get_vocab(thu_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.475585Z",
     "start_time": "2020-07-06T15:57:55.443638Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "标签编码\n",
    "'''\n",
    "labels_set = list(set(jieba_labels))\n",
    "labels_set.sort()\n",
    "labels_dict = dict([(lab,idx) for idx,lab in enumerate(labels_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.521463Z",
     "start_time": "2020-07-06T15:57:55.476549Z"
    }
   },
   "outputs": [],
   "source": [
    "jieba_labels = [ labels_dict[i] for i in jieba_labels]\n",
    "#thu_labels = [ labels_dict[i] for i in thu_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.644138Z",
     "start_time": "2020-07-06T15:57:55.522461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 28031,\n",
       " 2: 39396,\n",
       " 3: 37568,\n",
       " 4: 27085,\n",
       " 5: 17672,\n",
       " 6: 35785,\n",
       " 7: 27058,\n",
       " 8: 41543,\n",
       " 9: 24984,\n",
       " 10: 21422,\n",
       " 11: 26909,\n",
       " 13: 19322,\n",
       " 14: 29300,\n",
       " 12: 340,\n",
       " 0: 6273}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cnt ={}\n",
    "for l in jieba_labels:\n",
    "    if l  in labels_cnt.keys():\n",
    "        labels_cnt[l] +=1\n",
    "    else:\n",
    "        labels_cnt[l]=1\n",
    "labels_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量\n",
    "#### 词袋\n",
    "- 做法：\n",
    "    - 将语料库中的所有词提取出来形成词汇表\n",
    "    - 按照某种顺序排列这些词\n",
    "    - 对于每个文档，统计每个词在文档中出现的次数\n",
    "- 形成文档-词条矩阵：每个元素体现的是词频\n",
    "- 文本相似度计算：\n",
    "    - 计算两个行向量之间的点积\n",
    "        - 缺陷：只捕捉相似部分，而其他地方不受影响\n",
    "    - 余弦相似度：\n",
    "        - 将两个向量之间的点积除以欧几里得范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:57:55.652079Z",
     "start_time": "2020-07-06T15:57:55.646098Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_bag(corpus,word_to_index):\n",
    "    seqs = []\n",
    "    vocab_size = 2000\n",
    "    for line in  corpus:\n",
    "        seq = [0]*vocab_size \n",
    "        for word in line:\n",
    "            try:\n",
    "                seq[word_to_index.get(word)]+=1 \n",
    "            except TypeError:\n",
    "                pass \n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:58:53.707011Z",
     "start_time": "2020-07-06T15:57:55.655072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jb_wordbag_seqs = words_bag(jieba_texts,jb_word_to_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:29:38.872488Z",
     "start_time": "2020-07-06T14:29:38.850723Z"
    }
   },
   "outputs": [],
   "source": [
    "#thu_wordbag_seqs = words_bag(thu_texts,thu_word_to_index)  \n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- 词袋的局限性：\n",
    "    - 将每个词的重要性同等对待\n",
    "    - 实际上，在一个语料库中，某些词出现的频率会较高，会使得结果产生偏倚。\n",
    "- TF-IDF:\n",
    "     - 统计语料库中，含有每个词的文档数量（文档频率）\n",
    "     - 然后将词频除以该词条的文档频率\n",
    "     - 能够突出在文档中具有唯一性的词\n",
    "- 还存在几种变形：标准化或者平滑处理等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:58:53.738213Z",
     "start_time": "2020-07-06T15:58:53.707011Z"
    }
   },
   "outputs": [],
   "source": [
    "idf =[1]*2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:58:55.505206Z",
     "start_time": "2020-07-06T15:58:53.738213Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "内存有限，只看jieba的\n",
    "'''\n",
    "for line in jieba_texts:\n",
    "    wordset = set(line)\n",
    "    for word in wordset :\n",
    "        if jb_word_to_index.get(word):\n",
    "            idf[jb_word_to_index.get(word)]+=1 \n",
    "idf = np.array(idf).reshape(1,2000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:01:09.642117Z",
     "start_time": "2020-07-06T15:58:55.505206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "jb_wordbag_seqs = np.array(jb_wordbag_seqs )\n",
    "gc.collect()\n",
    "idf_seqs = jb_wordbag_seqs/idf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:15:07.843456Z",
     "start_time": "2020-07-06T06:15:07.816693Z"
    }
   },
   "source": [
    "\n",
    "#### one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:01:12.169250Z",
     "start_time": "2020-07-06T16:01:09.686844Z"
    }
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for line in jieba_texts:\n",
    "    seq = []\n",
    "    for word in line :\n",
    "        if jb_word_to_index.get(word):\n",
    "            seq.append(jb_word_to_index.get(word))\n",
    "    seqs.append(seq)\n",
    "\n",
    "max_len = max(len(seq) for seq in seqs)\n",
    "\n",
    "'''\n",
    "padding\n",
    "'''\n",
    "padding_seqs=[]\n",
    "for line in seqs:\n",
    "    padding_seqs.append(line+[0]*(max_len-len(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词嵌入\n",
    "- word2vec：词嵌入框架之一\n",
    "    - 核心概念：\n",
    "        - 一个模型能根据相邻词预测中心词，反之亦然；从而能捕捉上下文的意思。\n",
    "    - 每个词的意义分布在向量中，十分稳健。\n",
    "    - 词向量大小为超参数（sk）。\n",
    "    - 词袋则不一样，大小会随着数量增加而增加\n",
    "    - 一旦预训练了一组词向量，便可有效使用，无需转换\n",
    "    - 可作为RNN网络的输入向量\n",
    "    - 看我的cs224笔记\n",
    "- Glove:\n",
    "    - 词表征法的Glove,全局向量。\n",
    "    - 利用共现统计方法，直接优化每个词的向量表示法。\n",
    "    - 实现：\n",
    "        - 对于语料中所有的词对i,j，计算词j在词i的上下文出现的概率p(j|i);（j在i上下文出现：窗口内）。\n",
    "        - 对每个词初始化两个随机向量，一个代表上下文$\\widetilde{w}$，一个代表中心词$w$。\n",
    "        - 对于任何词对i,j，想让他的词向量的点积$\\widetilde{w_{i}}*w_{j}$等于共现概率\n",
    "        - 以此为目标，利用恰当的损失函数，通过迭代的方式优化这些词向量。\n",
    "        - 结果为一组词向量，捕捉了各个词之间的相似性与差异性\n",
    "     - 另一个角度：\n",
    "         实际上是将共现概率矩阵分解为两个小矩阵\n",
    "     \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:53:48.248606Z",
     "start_time": "2020-07-06T16:53:48.244616Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "直接调包~\n",
    "'''\n",
    "import gensim, logging, os\n",
    "import logging  \n",
    "import random\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os.path\n",
    "import sys \n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:04:42.058247Z",
     "start_time": "2020-07-06T17:04:41.230430Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_ =[]\n",
    "for idx,line in enumerate(jieba_texts):\n",
    "    corpus_.append([line,jieba_labels[idx]])\n",
    "random.shuffle(corpus_)\n",
    "\n",
    "###只取100000个样本\n",
    "corpus,labels = [],[]\n",
    "for line in corpus_[:100000]:\n",
    "    corpus.append(line[0])\n",
    "    labels.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T17:05:04.393714Z",
     "start_time": "2020-07-06T17:05:03.782164Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('w2v_corpus.txt','w',encoding='utf-8') as fW:\n",
    "    for line in corpus:\n",
    "        fW.write(' '.join(line))\n",
    "        fW.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T21:43:44.214162Z",
     "start_time": "2020-07-06T17:05:11.463208Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "out_model = 'sk.model'\n",
    "out_vector = 'sk.vector'\n",
    "# 训练skip-gram模型\n",
    "sentences = word2vec.LineSentence('w2v_corpus.txt') \n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=2,window=5,size=100,sg=1)\n",
    "# 保存模型\n",
    "model.save(out_model)\n",
    "# 保存词向量\n",
    "model.wv.save_word2vec_format(out_vector, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-06T17:20:52.199Z"
    }
   },
   "outputs": [],
   "source": [
    "out_model = 'cb.model'\n",
    "out_vector = 'cb.vector'\n",
    "# 训练cbow模型\n",
    "sentences = word2vec.LineSentence('w2v_corpus.txt') \n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=2,window=5,size=100,sg=2)\n",
    "model.save(out_model)\n",
    "model.wv.save_word2vec_format(out_vector, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类任务\n",
    " - 暂时没做完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
