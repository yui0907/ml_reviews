{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以头条新闻的分类数据集为语料来实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "#### 读取数据与分词\n",
    "\n",
    "- 用两种工具处理：jieba和thulac,进行分词\n",
    "- 然后去除停用词，用的百度停用词数据集\n",
    "\n",
    "**Todo**：\n",
    "\n",
    "    1.分词的效果不咋样啊，很多专有名词识别不出来\n",
    "    2.类别不均衡，需要增强\n",
    "\n",
    "本次只比较不同词向量的效果，暂时不做这些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:14:55.221795Z",
     "start_time": "2020-07-06T14:14:53.890128Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "import thulac\n",
    "import re \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:14:58.074988Z",
     "start_time": "2020-07-06T14:14:57.418049Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "filename = 'toutiao_cat_data.txt'\n",
    "thu1 = thulac.thulac(seg_only=True) \n",
    "stopwords =[ line.rstrip() for line in open('stopwords-master/baidu_stopwords.txt',encoding='utf-8') ]\n",
    "\n",
    "def jieba_pre(text):\n",
    "    seg_list = jieba.cut(text, cut_all=False,HMM=True)\n",
    "    return [seg for seg in seg_list if seg not in stopwords]\n",
    "\n",
    "def thu_pre(text):\n",
    "     #默认模式是分词+词性标注，这里只分词\n",
    "    seg_list = thu1.cut(text, text=True)  #进行一句话分词\n",
    "    return [seg for seg in seg_list.split(' ')  if seg not in stopwords]\n",
    "\n",
    "def preprocess(pre_model,filename):\n",
    "    texts,labels =[],[]\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f.readlines():\n",
    "            s = l.strip().split('_!_')\n",
    "            if len(s) >=4 :\n",
    "                ss = re.sub('[0-9]+',' ',s[3])\n",
    "                texts.append(pre_model(ss)+s[4].split(','))\n",
    "                labels.append(s[1])\n",
    "    return texts,labels \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:24:56.758343Z",
     "start_time": "2020-07-06T14:15:02.560993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yui\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.742 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba_texts,jieba_labels = preprocess(jieba_pre,filename)\n",
    "thu_texts,thu_labels = preprocess(thu_pre,filename)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:26:57.677158Z",
     "start_time": "2020-07-06T12:23:46.631251Z"
    }
   },
   "outputs": [],
   "source": [
    "#jieba_texts,jieba_labels = preprocess(jieba_pre,filename)\n",
    "#thu_texts,thu_labels = preprocess(thu_pre,filename)\n",
    "'''\n",
    "好慢！以防万一导出到本地\n",
    "\n",
    "jieba_seq=[]\n",
    "for i in range(len(jieba_texts)):\n",
    "    jieba_seq.append(','.join(jieba_texts[i])+','+jieba_labels[i])\n",
    "\n",
    "thu_seq=[]\n",
    "for i in range(len(jieba_texts)):\n",
    "    thu_seq.append(','.join(thu_texts[i])+','+thu_labels[i])\n",
    "\n",
    "    \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.DataFrame(data=np.array([jieba_seq,thu_seq]).transpose(),columns=['jieba_seq','thu_seq'])\n",
    "df.to_csv('fenci_pre.csv',encoding='utf-8')\n",
    "\n",
    "\n",
    "df = pd.read_csv('fenci_pre.csv',encoding='utf-8')\n",
    "jieba_texts,jieba_labels =[],[]\n",
    "thu_texts,thu_labels=[],[]\n",
    "for i in range(df.shape[0]):\n",
    "    jieba_texts.append(df.iloc[i][1].split(',')[:-1])\n",
    "    thu_texts.append(df.iloc[i][2].split(',')[:-1])\n",
    "    jieba_labels.append(df.iloc[i][1].split(',')[-1])\n",
    "    thu_labels.append(df.iloc[i][2].split(',')[-1])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:43:08.244665Z",
     "start_time": "2020-07-06T06:43:08.230607Z"
    }
   },
   "source": [
    "#### 词表字典\n",
    "\n",
    "这一步得到三个产物：\n",
    "\n",
    "    单词计数\n",
    "    单词到索引\n",
    "    索引到单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:25.024536Z",
     "start_time": "2020-07-06T14:25:25.016549Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    word_counts = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "\n",
    "\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            if word in word_counts and len(word)>1  :\n",
    "                word_counts[word]+=1\n",
    "            elif len(word)>1 :\n",
    "                word_counts[word] = 1 \n",
    "    word_counts=dict(sorted(word_counts.items(),key=lambda x:x[1],reverse=True)[:2000])\n",
    "    for  idx,item in enumerate(word_counts.items()):\n",
    "        word_to_index[item[0]] = idx \n",
    "    \n",
    "    index_to_word = dict([ (v,k) for k,v in word_to_index.items() ])\n",
    "\n",
    "    return word_counts,word_to_index,index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:30.859335Z",
     "start_time": "2020-07-06T14:25:29.280359Z"
    }
   },
   "outputs": [],
   "source": [
    "jb_word_counts,jb_word_to_index,jb_index_to_word = get_vocab(jieba_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:32.779292Z",
     "start_time": "2020-07-06T14:25:32.776301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#thu_word_counts,thu_word_to_index,thu_index_to_word = get_vocab(thu_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:33.235205Z",
     "start_time": "2020-07-06T14:25:33.207312Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "标签编码\n",
    "'''\n",
    "labels_set = list(set(jieba_labels))\n",
    "labels_set.sort()\n",
    "labels_dict = dict([(lab,idx) for idx,lab in enumerate(labels_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:39.946873Z",
     "start_time": "2020-07-06T14:25:39.897040Z"
    }
   },
   "outputs": [],
   "source": [
    "jieba_labels = [ labels_dict[i] for i in jieba_labels]\n",
    "#thu_labels = [ labels_dict[i] for i in thu_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:25:42.310692Z",
     "start_time": "2020-07-06T14:25:42.215339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 28031,\n",
       " 2: 39396,\n",
       " 3: 37568,\n",
       " 4: 27085,\n",
       " 5: 17672,\n",
       " 6: 35785,\n",
       " 7: 27058,\n",
       " 8: 41543,\n",
       " 9: 24984,\n",
       " 10: 21422,\n",
       " 11: 26909,\n",
       " 13: 19322,\n",
       " 14: 29300,\n",
       " 12: 340,\n",
       " 0: 6273}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cnt ={}\n",
    "for l in jieba_labels:\n",
    "    if l  in labels_cnt.keys():\n",
    "        labels_cnt[l] +=1\n",
    "    else:\n",
    "        labels_cnt[l]=1\n",
    "labels_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量\n",
    "##### 词袋\n",
    "- 做法：\n",
    "    - 将语料库中的所有词提取出来形成词汇表\n",
    "    - 按照某种顺序排列这些词\n",
    "    - 对于每个文档，统计每个词在文档中出现的次数\n",
    "- 形成文档-词条矩阵：每个元素体现的是词频\n",
    "- 文本相似度计算：\n",
    "    - 计算两个行向量之间的点积\n",
    "        - 缺陷：只捕捉相似部分，而其他地方不受影响\n",
    "    - 余弦相似度：\n",
    "        - 将两个向量之间的点积除以欧几里得范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:26:07.351407Z",
     "start_time": "2020-07-06T14:26:07.344462Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_bag(corpus,word_to_index):\n",
    "    seqs = []\n",
    "    vocab_size = 2000\n",
    "    for line in  corpus:\n",
    "        seq = [0]*vocab_size \n",
    "        for word in line:\n",
    "            try:\n",
    "                seq[word_to_index.get(word)]+=1 \n",
    "            except TypeError:\n",
    "                pass \n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:26:41.169172Z",
     "start_time": "2020-07-06T14:26:09.927167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jb_wordbag_seqs = words_bag(jieba_texts,jb_word_to_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:29:38.872488Z",
     "start_time": "2020-07-06T14:29:38.850723Z"
    }
   },
   "outputs": [],
   "source": [
    "#thu_wordbag_seqs = words_bag(thu_texts,thu_word_to_index)  \n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "- 词袋的局限性：\n",
    "    - 将每个词的重要性同等对待\n",
    "    - 实际上，在一个语料库中，某些词出现的频率会较高，会使得结果产生偏倚。\n",
    "- TF-IDF:\n",
    "     - 统计语料库中，含有每个词的文档数量（文档频率）\n",
    "     - 然后将词频除以该词条的文档频率\n",
    "     - 能够突出在文档中具有唯一性的词\n",
    "- 还存在几种变形：标准化或者平滑处理等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:43:13.949910Z",
     "start_time": "2020-07-06T14:43:13.945921Z"
    }
   },
   "outputs": [],
   "source": [
    "idf =[1]*2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:43:22.594090Z",
     "start_time": "2020-07-06T14:43:19.799365Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "内存有限，只看jieba的\n",
    "'''\n",
    "for line in jieba_texts:\n",
    "    wordset = set(line)\n",
    "    for word in wordset :\n",
    "        if jb_word_to_index.get(word):\n",
    "            idf[jb_word_to_index.get(word)]+=1 \n",
    "idf = np.array(idf).reshape(1,2000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T14:46:18.227777Z",
     "start_time": "2020-07-06T14:45:47.516929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11007"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "jb_wordbag_seqs = np.array(jb_wordbag_seqs )\n",
    "gc.collect()\n",
    "idf_seqs = jb_wordbag_seqs/idf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:15:07.843456Z",
     "start_time": "2020-07-06T06:15:07.816693Z"
    }
   },
   "source": [
    "\n",
    "##### one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:03:39.737620Z",
     "start_time": "2020-07-06T15:03:38.300105Z"
    }
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for line in jieba_texts:\n",
    "    seq = []\n",
    "    for word in line :\n",
    "        if jb_word_to_index.get(word):\n",
    "            seq.append(jb_word_to_index.get(word))\n",
    "    seqs.append(seq)\n",
    "\n",
    "max_len = max(len(seq) for seq in seqs)\n",
    "\n",
    "'''\n",
    "padding\n",
    "'''\n",
    "padding_seqs=[]\n",
    "for line in seqs:\n",
    "    padding_seqs.append(line+[0]*(max_len-len(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 词嵌入\n",
    "- word2vec：词嵌入框架之一\n",
    "    - 核心概念：\n",
    "        - 一个模型能根据相邻词预测中心词，反之亦然；从而能捕捉上下文的意思。\n",
    "    - 每个词的意义分布在向量中，十分稳健。\n",
    "    - 词向量大小为超参数（sk）。\n",
    "    - 词袋则不一样，大小会随着数量增加而增加\n",
    "    - 一旦预训练了一组词向量，便可有效使用，无需转换\n",
    "    - 可作为RNN网络的输入向量\n",
    "    - 看我的cs224笔记\n",
    "- Glove:\n",
    "    - 词表征法的Glove,全局向量。\n",
    "    - 利用共现统计方法，直接优化每个词的向量表示法。\n",
    "    - 实现：\n",
    "        - 对于语料中所有的词对i,j，计算词j在词i的上下文出现的概率p(j|i);（j在i上下文出现：窗口内）。\n",
    "        - 对每个词初始化两个随机向量，一个代表上下文$\\widetilde{w}$，一个代表中心词$w$。\n",
    "        - 对于任何词对i,j，想让他的词向量的点积$\\widetilde{w_{i}}*w_{j}$等于共现概率\n",
    "        - 以此为目标，利用恰当的损失函数，通过迭代的方式优化这些词向量。\n",
    "        - 结果为一组词向量，捕捉了各个词之间的相似性与差异性\n",
    "     - 另一个角度：\n",
    "         实际上是将共现概率矩阵分解为两个小矩阵\n",
    "     \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:29:38.874233Z",
     "start_time": "2020-07-06T15:29:38.867217Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "直接掉包~\n",
    "'''\n",
    "import gensim, logging, os\n",
    "import logging  \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,filename='test_01.log')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:32:08.073471Z",
     "start_time": "2020-07-06T15:32:07.821724Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus =[]\n",
    "for line in jieba_texts:\n",
    "    corpus.append(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = word2vec.LineSentence(corpus) \n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=12,window=25,min_count=2,workers=5,sg=1,hs=1) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
